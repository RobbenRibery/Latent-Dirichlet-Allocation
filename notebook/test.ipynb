{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys \n",
    "if '/Users/ericliu/Desktop/Latent-Dirichilet-Allocation' not in sys.path: \n",
    "    sys.path.append('/Users/ericliu/Desktop/Latent-Dirichilet-Allocation')\n",
    "\n",
    "from collections import Counter, OrderedDict \n",
    "\n",
    "import torch as tr \n",
    "import numpy as np \n",
    "\n",
    "from src.utils import expec_log_dirichlet, expec_log_dirichlet_mirror, compute_elbo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import psi, polygamma, gammaln\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/ericliu/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "import nltk \n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "stops += [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\", \".\", \"!\", \"?\", \",\", \";\", \":\", \"[\", \"]\", \"{\", \"}\", \"-\", \"+\", \n",
    "    \"_\", \"/\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"(\", \")\", \"<\", \">\", \"|\", \"=\",\n",
    "    \".-\", \".,\", \"'\", '\"', ',\"'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dg(gamma, d, i):\n",
    "    \"\"\"\n",
    "    E[log θ_t] where θ_t ~ Dir(gamma)\n",
    "    \"\"\"\n",
    "    return psi(gamma[d, i]) - psi(np.sum(gamma[d, :]))\n",
    "\n",
    "\n",
    "def dl(lam, i, w_n):\n",
    "    \"\"\"\n",
    "    E[log β_t] where β_t ~ Dir(lam)\n",
    "    \"\"\"\n",
    "    return psi(lam[i, w_n]) - psi(np.sum(lam[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words()[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch titles only\n",
    "# fetch 2000 docs only\n",
    "trainset, testset = [], []\n",
    "vocab = []\n",
    "\n",
    "i = 0\n",
    "for file_id in reuters.fileids():\n",
    "    if file_id.startswith(\"train\"):\n",
    "        doc = [w.lower() for w in reuters.words(file_id) \\\n",
    "                 if (w.isupper()) \\\n",
    "                 if (w.lower() not in stops) \\\n",
    "                 and (not w.isnumeric())]\n",
    "        if doc:\n",
    "            trainset.append(doc)\n",
    "            vocab += doc\n",
    "            i += 1\n",
    "    else:\n",
    "        testset.append(\n",
    "            [w.lower() for w in reuters.words(file_id) \\\n",
    "                         if (w.isupper()) \\\n",
    "                         and (w.lower() not in stops) \\\n",
    "                         and (not w.isnumeric())]\n",
    "        )\n",
    "    if i >= 2000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(vocab))\n",
    "word_to_ix = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4048\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2000 observations in trainset\n",
      "['bahia', 'cocoa', 'review', 'fob', 'u', 'u']\n",
      "There are 3019 observations in testset\n",
      "['asian', 'exporters', 'fear', 'damage', 'u', 'japan', 'rift', 'u', 'u', 'u', 'u', 'mc', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'miti']\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(trainset)} observations in trainset\")\n",
    "print(trainset[0])\n",
    "\n",
    "print(f\"There are {len(testset)} observations in testset\")\n",
    "print(testset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476\n",
      "2151\n",
      "2383\n",
      "3925\n",
      "1232\n",
      "1232\n"
     ]
    }
   ],
   "source": [
    "for key in trainset[0]:\n",
    "    print(word_to_ix[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_ix(seq, vocab=vocab):\n",
    "    # len(vocab), which is the last index, is for the <unk> (unknown) token\n",
    "    unk_idx = len(vocab)\n",
    "    return np.array(list(map(lambda w: word_to_ix.get(w, unk_idx), seq)))\n",
    "\n",
    "data = {\n",
    "    \"train\": list(map(seq_to_ix, trainset)),\n",
    "    \"test\": list(map(seq_to_ix, testset))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1476, 2151, 2383, 3925, 1232, 1232])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedLDA:\n",
    "    \n",
    "    def __init__(self, docs, vocab, k):\n",
    "        self.docs = docs\n",
    "        \n",
    "        self.V = len(vocab)\n",
    "        self.k = k  # number of topics\n",
    "        self.N = np.array([doc.shape[0] for doc in docs])\n",
    "        self.M = len(docs)\n",
    "        \n",
    "        V = self.V\n",
    "        N = self.N\n",
    "        M = self.M\n",
    "        \n",
    "        # initialize model parameters\n",
    "        ##self.beta = np.ones((k, V)) / V\n",
    "        self.alpha = np.random.gamma(100, 0.01, k)\n",
    "        self.eta = np.ones(V)\n",
    "\n",
    "        # initialize variational parameters\n",
    "        # ϕ: (M x max(N) x k) arrays with zero paddings on the right\n",
    "        self.phi = [np.ones((N[d], k)) / k for d in range(M)]\n",
    "        self.gamma = self.alpha + (N / k).reshape(-1, 1)\n",
    "        self.lam = np.random.gamma(shape=100, scale=0.01, size=(k, V))\n",
    "\n",
    "    def vlb(self):\n",
    "        \"\"\"\n",
    "        lower bound from variational inference\n",
    "        \"\"\"\n",
    "        phi = self.phi\n",
    "        gamma = self.gamma\n",
    "        lam = self.lam\n",
    "        alpha = self.alpha\n",
    "        eta = self.eta\n",
    "        docs = self.docs\n",
    "        \n",
    "        M = self.M\n",
    "        k = self.k\n",
    "        N = self.N\n",
    "        V = self.V\n",
    "        \n",
    "        a0, a1, a2, a3_1, a3_2, a4, a5 = 0., 0., 0., 0., 0., 0., 0.\n",
    "\n",
    "        a0 += (\n",
    "            k * (\n",
    "                gammaln(np.sum(eta)) \n",
    "                - np.sum(gammaln(eta))\n",
    "            )\n",
    "            + np.sum([(eta[j] - 1) * dl(lam, i, j) for j in range(V) for i in range(k)])\n",
    "        )\n",
    "        for d in range(M):\n",
    "\n",
    "            a1 += (\n",
    "                gammaln(np.sum(alpha))\n",
    "                - np.sum(gammaln(alpha))\n",
    "                + np.sum([(alpha[i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "            )\n",
    "\n",
    "            a4 += (\n",
    "                gammaln(np.sum(gamma[d, :]))\n",
    "                - np.sum(gammaln(gamma[d, :]))\n",
    "                + np.sum([(gamma[d, i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "            )\n",
    "            \n",
    "            for i in range(k):\n",
    "                for j in range(V):\n",
    "                    a3_2 += (\n",
    "                        gammaln(np.sum(lam[i, j]))\n",
    "                        - np.sum(gammaln(lam[i, :]))\n",
    "                        + np.sum((lam[i, j] - 1) * dl(lam, i, j))\n",
    "                    )\n",
    "\n",
    "            for n in range(N[d]):\n",
    "                w_n = int(docs[d][n])\n",
    "                a2 += np.sum([phi[d][n, i] * dg(gamma, d, i) for i in range(k)])\n",
    "                a3_1 += np.sum([phi[d][n, i] * dl(lam, i, w_n) for i in range(k)])\n",
    "                a5 += np.sum([phi[d][n, i] * np.log(phi[d][n, i]) for i in range(k)])\n",
    "\n",
    "        return a0 + a1 + a2 + a3_1 - a3_2 - a4 - a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bahia', 'cocoa', 'review', 'fob', 'u', 'u']\n",
      "[array([1476, 2151, 2383, 3925, 1232, 1232])]\n"
     ]
    }
   ],
   "source": [
    "test_docs_str = trainset[0]\n",
    "test_docs = [data['train'][0]]\n",
    "\n",
    "print(test_docs_str)\n",
    "print(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirror_model = SmoothedLDA(\n",
    "    docs= test_docs,\n",
    "    vocab = vocab, \n",
    "    k = 10, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1667503.6951637845"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mirror_model.vlb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n"
     ]
    }
   ],
   "source": [
    "print(len(mirror_model.phi))\n",
    "print(mirror_model.phi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bahia': 0, 'cocoa': 0, 'review': 0, 'fob': 0, 'u': 0}\n",
      "{'bahia': [0], 'cocoa': [1], 'review': [2], 'fob': [3], 'u': [4, 5]}\n"
     ]
    }
   ],
   "source": [
    "word_cts = {}\n",
    "word_pos = {}\n",
    "\n",
    "for index, word in enumerate(test_docs_str): \n",
    "\n",
    "    \n",
    "    word_cts[word]=word_cts.get(word, 0)\n",
    "\n",
    "    if word not in word_pos: \n",
    "\n",
    "        word_pos[word] = []\n",
    "\n",
    "    word_pos[word].append(index)\n",
    "\n",
    "print(word_cts)\n",
    "print(word_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'u': 2, 'bahia': 1, 'cocoa': 1, 'review': 1, 'fob': 1})\n"
     ]
    }
   ],
   "source": [
    "word_cts = Counter(test_docs_str)\n",
    "word_inds = word_to_ix\n",
    "\n",
    "print(word_cts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4048)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mirror_model.lam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mirror_model.gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(len(mirror_model.phi)):\n",
    "    if not isinstance(mirror_model.phi[doc], tr.Tensor):\n",
    "        mirror_model.phi[doc] = tr.tensor(mirror_model.phi[doc], dtype=float)\n",
    "#mirror_model.phi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-283.6431, dtype=torch.float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_elbo(\n",
    "    batch_size=len(test_docs), \n",
    "    _gamma_ = tr.tensor(mirror_model.gamma, dtype=float), \n",
    "    _phi_ = mirror_model.phi, \n",
    "    _lambda_ = tr.tensor(mirror_model.lam, dtype=float),\n",
    "    _alpha_ = tr.tensor(mirror_model.alpha, dtype=float),\n",
    "    _eta_ = tr.tensor(mirror_model.eta, dtype=float),\n",
    "    word_cts = [word_cts],\n",
    "    word_pos = [word_pos],\n",
    "    word_to_inds = word_to_ix,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e026e684521c612822fbb31521a7b705da4f7f2b939b93351f3c885564d078c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
